{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "mount_file_id": "1sZLWBh-U5HBa3TenmsszMDsXxJvLxEng",
   "authorship_tag": "ABX9TyO/gUbQ6Y3XMY2D26rzOoUR",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stefanmzeidler/HI744_Programming_Assignment_1/blob/main/Zeidler_Programming_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installs, downloads, and imports"
   ],
   "metadata": {
    "id": "q1PeB9VMUj4e"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qQ3idEXQXdo",
    "outputId": "678e84fc-de77-42c7-df62-17c51a0a411f",
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:14.937223Z",
     "start_time": "2025-11-23T12:03:13.042642Z"
    }
   },
   "source": [
    "!pip install gensim\n",
    "!pip install nltk"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from gensim) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\school\\anaconda3\\envs\\hi744_assignment1\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44ttlXJaTpFw",
    "outputId": "522002ba-1de6-4839-a311-8d202cde5137",
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:16.415970Z",
     "start_time": "2025-11-23T12:03:14.941875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\School\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\School\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Util Functions"
   ],
   "metadata": {
    "id": "Hw00rjzyVMqG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Provided by Professor He\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def read_txt_files_from_directory(directory_path):\n",
    "    file_contents = {}\n",
    "    try:\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        file_contents[filename] = file.read()\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while reading {filename}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while accessing the directory: {e}\")\n",
    "        return {}\n",
    "    return file_contents\n",
    "\n",
    "def load_from_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_sentence.append(token)\n",
    "    return filtered_sentence\n",
    "\n",
    "def stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def pre_process(text):\n",
    "    text_lower = text.lower()\n",
    "    tokens_no_punctuation = remove_punctuation(text_lower)\n",
    "    filtered_tokens = remove_stop_words(tokens_no_punctuation)\n",
    "    stemmed_tokens = stemming(filtered_tokens)\n",
    "    return stemmed_tokens"
   ],
   "metadata": {
    "id": "LWp86ThuVSbe",
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:16.426573Z",
     "start_time": "2025-11-23T12:03:16.420477Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def top_5(vector_matrix, data, column_name):\n",
    "  \"\"\"\n",
    "  Inserts the top 5 most similar indices for a given matrix of vectors into a dataframe.\n",
    "  :param vector_matrix: Matrix of vectors.\n",
    "  :param data: Dataframe to insert top5 list.\n",
    "  :param column_name: Name for new column containing the top 5.\n",
    "  \"\"\"\n",
    "  sim_matrix = cosine_similarity(vector_matrix)\n",
    "  top5_indices = np.argpartition(-sim_matrix, range(6), axis=1)[:, 1:6].tolist()\n",
    "  data[column_name] = top5_indices\n",
    "  data[column_name] = data[column_name].apply(lambda x: index_to_id(data, x))\n",
    "\n",
    "def index_to_id(data, index_list)->list[str]:\n",
    "  \"\"\"\n",
    "  Given a list of indices, returns the patient UID from a given dataframe.\n",
    "  :param data:Dataframe containing the patient UID.`\n",
    "  :param index_list:List of indices for each patient\n",
    "  :return:List of patient_ID strings\n",
    "  \"\"\"\n",
    "  id_list = []\n",
    "  for i in index_list:\n",
    "    id_list.append(data.at[i,'patient_uid'])\n",
    "  return id_list\n"
   ],
   "metadata": {
    "id": "gfqdR3Ne8p00",
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:16.434353Z",
     "start_time": "2025-11-23T12:03:16.430500Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF Similarity"
   ],
   "metadata": {
    "id": "ejD8_ufEwCzU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_similarity(data):\n",
    "  tfidf = TfidfVectorizer()\n",
    "  vector_matrix = tfidf.fit_transform(data['patient'])\n",
    "  top_5(vector_matrix, data, 'similar_patients_tfidf')"
   ],
   "metadata": {
    "id": "WwuQ3gU-wGIp",
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:16.440439Z",
     "start_time": "2025-11-23T12:03:16.437908Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Doc2Vec Similarity"
   ],
   "metadata": {
    "id": "D1fQVKosv-xB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "def load_dataset(fname:str,nrows = None):\n",
    "  data = safe_read_csv(fname,nrows)\n",
    "  data['tokens'] = data['patient'].apply(lambda text: pre_process(text))\n",
    "  print(\"Data loaded\")\n",
    "  return data\n",
    "\n",
    "def safe_read_csv(fname:str,nrows):\n",
    "  try:\n",
    "    filepath = os.path.join(PROJ_DIRECTORY,fname)\n",
    "    return pd.read_csv(filepath_or_buffer=filepath,nrows=nrows)\n",
    "  except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    return None\n",
    "\n",
    "#Adapted from https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py\n",
    "def read_corpus(data, tokens_only = False):\n",
    "    documents = data['tokens']\n",
    "    for i, tokens in documents.items():\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "def train_doc2vec(data):\n",
    "  model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "  train_corpus = list(read_corpus(data))\n",
    "  model.build_vocab(train_corpus)\n",
    "  print(\"Vocab built\")\n",
    "  print(\"Starting training\")\n",
    "  model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "  fname = get_tmpfile(os.path.join(PROJ_DIRECTORY,\"my_doc2vec_model\"))\n",
    "  model.save(fname)\n",
    "  print(\"Model saved\")\n",
    "  return model\n",
    "\n",
    "def doc2vec_vectors(model, data):\n",
    "  vector_matrix = []\n",
    "  print(\"Calcualting Doc2Vec vectors\")\n",
    "  for _ , tokens in data['tokens'].items():\n",
    "    vector_matrix.append(np.array(model.infer_vector(tokens)))\n",
    "  print(\"Vectors calculated\")\n",
    "  return vector_matrix\n",
    "\n",
    "\n",
    "def load_model(fname):\n",
    "  return Doc2Vec.load(fname)\n",
    "\n",
    "def doc2vec_similarity(data):\n",
    "  doc2vec_model = train_doc2vec(data)\n",
    "  vector_matrix = doc2vec_vectors(doc2vec_model,data)\n",
    "  top_5(vector_matrix, data,'similar_patients_doc2vec')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "9OBmUsM-WgAn",
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:16.622764Z",
     "start_time": "2025-11-23T12:03:16.444186Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:16.630079Z",
     "start_time": "2025-11-23T12:03:16.627196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gen_similar_patients(data):\n",
    "    similar_patients = data[['patient_uid','similar_patients_tfidf','similar_patients_doc2vec']]\n",
    "    similar_patients.to_json('similar_patients.json',orient='records')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:16.637660Z",
     "start_time": "2025-11-23T12:03:16.633284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score\n",
    "import ast\n",
    "def generate_metrics(data):\n",
    "    data['similar_patients'] = data['similar_patients'].apply(lambda x: string_to_list(x))\n",
    "    data['doc2vec_precision'],data['doc2vec_recall'] = zip(*data.apply(lambda row:calc_precision_recall(row['similar_patients'],row['similar_patients_doc2vec']), axis=1))\n",
    "    data['tfidf_precision'],data['tfidf_recall'] = zip(*data.apply(lambda row:calc_precision_recall(row['similar_patients'],row['similar_patients_tfidf']), axis=1))\n",
    "    print(data.head())\n",
    "\n",
    "def string_to_list(text):\n",
    "    if type(text) == list:\n",
    "        return text\n",
    "    else:\n",
    "        return list(ast.literal_eval(text).keys())\n",
    "def calc_precision_recall(y_true, y_pred):\n",
    "    true_positives = len([patient for patient in y_pred if patient in y_true])\n",
    "    false_positives = len(y_pred) - true_positives\n",
    "    false_negatives = len(y_true) - true_positives\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = 0 if true_positives + false_negatives  == 0 else true_positives / (true_positives + false_negatives)\n",
    "    return precision,recall\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "PROJ_DIRECTORY =os.getcwd()\n",
    "dataset = load_dataset(\"PMC-Patients.csv\",nrows=1000)\n",
    "doc2vec_similarity(dataset)\n",
    "tfidf_similarity(dataset)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YVOmNrXX1bq",
    "outputId": "8e973949-b877-4490-b4c0-7056d17c6db4",
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:40.477671Z",
     "start_time": "2025-11-23T12:03:16.642975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Vocab built\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "Calcualting Doc2Vec vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors calculated\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T12:03:40.533845Z",
     "start_time": "2025-11-23T12:03:40.484218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gen_similar_patients(dataset)\n",
    "generate_metrics(dataset)\n",
    "# extract_keys(dataset.at[1,'similar_patients'])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patient_id patient_uid      PMID                         file_path  \\\n",
      "0           0   7665777-1  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
      "1           1   7665777-2  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
      "2           2   7665777-3  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
      "3           3   7665777-4  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
      "4           4   7665777-5  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
      "\n",
      "                                               title  \\\n",
      "0  Early Physical Therapist Interventions for Pat...   \n",
      "1  Early Physical Therapist Interventions for Pat...   \n",
      "2  Early Physical Therapist Interventions for Pat...   \n",
      "3  Early Physical Therapist Interventions for Pat...   \n",
      "4  Early Physical Therapist Interventions for Pat...   \n",
      "\n",
      "                                             patient               age gender  \\\n",
      "0  This 60-year-old male was hospitalized due to ...  [[60.0, 'year']]      M   \n",
      "1  A 39-year-old man was hospitalized due to an i...  [[39.0, 'year']]      M   \n",
      "2  One week after a positive COVID-19 result this...  [[57.0, 'year']]      M   \n",
      "3  This 69-year-old male was admitted to the ICU ...  [[69.0, 'year']]      M   \n",
      "4  This 57-year-old male was admitted to the ICU ...  [[57.0, 'year']]      M   \n",
      "\n",
      "                                   relevant_articles  \\\n",
      "0  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
      "1  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
      "2  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
      "3  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
      "4  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
      "\n",
      "                                    similar_patients  \\\n",
      "0  [7665777-2, 7665777-3, 7665777-4, 7665777-5, 7...   \n",
      "1  [7665777-1, 7665777-3, 7665777-4, 7665777-5, 7...   \n",
      "2  [7665777-1, 7665777-2, 7665777-4, 7665777-5, 7...   \n",
      "3  [7665777-1, 7665777-2, 7665777-3, 7665777-5, 7...   \n",
      "4  [7665777-1, 7665777-2, 7665777-3, 7665777-4, 7...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [60, year, old, male, hospit, due, moder, ard,...   \n",
      "1  [39, year, old, man, hospit, due, increasingli...   \n",
      "2  [one, week, posit, covid, 19, result, 57, year...   \n",
      "3  [69, year, old, male, admit, icu, dri, cough, ...   \n",
      "4  [57, year, old, male, admit, icu, dyspnea, hea...   \n",
      "\n",
      "                            similar_patients_doc2vec  \\\n",
      "0  [7665777-10, 7665777-2, 7665777-3, 7665777-5, ...   \n",
      "1  [7665777-10, 7665777-3, 7665777-5, 7665777-7, ...   \n",
      "2  [7665777-1, 7665777-10, 7665777-2, 7665777-7, ...   \n",
      "3  [7665777-10, 7665777-2, 7665777-3, 7665777-8, ...   \n",
      "4  [7665777-10, 7665777-2, 7665777-3, 7665777-7, ...   \n",
      "\n",
      "                              similar_patients_tfidf  doc2vec_precision  \\\n",
      "0  [6011174-1, 7665777-2, 7665777-3, 7665777-7, 8...                1.0   \n",
      "1  [6004679-1, 6005892-1, 6011170-1, 7665777-1, 7...                1.0   \n",
      "2  [6011174-1, 7665777-1, 7665777-2, 7665777-7, 8...                1.0   \n",
      "3  [7665777-10, 7665777-3, 7665777-7, 7665777-9, ...                1.0   \n",
      "4  [6029745-1, 7665777-10, 7665777-11, 7665777-3,...                1.0   \n",
      "\n",
      "   doc2vec_recall  tfidf_precision  tfidf_recall  \n",
      "0             0.5              0.6           0.3  \n",
      "1             0.5              0.4           0.2  \n",
      "2             0.5              0.6           0.3  \n",
      "3             0.5              0.8           0.4  \n",
      "4             0.5              0.8           0.4  \n"
     ]
    }
   ],
   "execution_count": 10
  }
 ]
}
